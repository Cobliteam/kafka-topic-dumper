import argparse
import logging
import tempfile
import sys
from datetime import datetime

from kafka_topic_dumper.kafka_client import KafkaClient


logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def parse_command_line():
    parser = argparse.ArgumentParser(description='Simple tool to dump kafka '
                                                 'messages and send it to AWS '
                                                 'S3')

    parser.add_argument('-t', '--topic', default='test',
                        help='Kafka topic to fetch messages from.')

    parser.add_argument('-s', '--bootstrap-servers', default='localhost:9092',
                        help='host[:port] string (or list of host[:port] '
                             'strings) that the consumer should contact to '
                             'bootstrap initial cluster metadata. If no '
                             'servers are specified, will default to '
                             'localhost:9092.')

    parser.add_argument('-b', '--bucket-name', default='kafka-topic-dumper',
                        help='The AWS-S3 bucket name to send dump files.')

    parser.add_argument('-p', '--path', default=None,
                        help='Path to folder where to store local files.')

    parser.add_argument('-x', '--prefix', default=None,
                        help='The name of the folder to dump/reload.')

    cmds = parser.add_subparsers(help='sub-command help')

    dump_cmd = cmds.add_parser('dump',
                               help='Dump mode will fetch messages from kafka '
                                    'cluster and send then to AWS-S3.')

    dump_cmd.add_argument('-n', '--num-messages', default=300, type=int,
                          help='Number of messages to try dump.')

    dump_cmd.add_argument('-m', '--max-messages-per-package', default=100,
                          type=int,
                          help='Maximum number of messages per dump file.')

    dump_cmd.add_argument('-d', '--dry-run', action='store_true',
                          help='In dry run mode, kafka-topic-dumper will '
                               'generate local files. But will not send it to '
                               'AWS S3 bucket.')

    dump_cmd.set_defaults(action='dump')

    reload_cmd = cmds.add_parser('reload',
                                 help='Reload mode will download files from '
                                      'AWS-S3 and send then to kafka.')

    reload_cmd.add_argument('-g', '--reload-consumer-group',
                            default=None,
                            help='Whe reloading a dump of messages that '
                                 'already was in kafka, kafka-topic-dumper '
                                 'will not load it again, it will only reset '
                                 'offsets for this consumer-group.')

    reload_cmd.add_argument('-T', '--transformer',
                            default='kafka_topic_dumper.transformer:Identity',
                            help='package:class that will be used to transform '
                                 'each message before producing')

    reload_cmd.set_defaults(action='reload')

    opts = parser.parse_args()

    if getattr(opts, 'action', None) is None:
        parser.print_help()
        sys.exit(1)

    print(opts)

    return opts


def get_transformer_class(pre_processing):
    [module_name, class_name] = pre_processing.split(":")

    logger.info(module_name)
    module = __import__(module_name, globals(), locals(), [class_name], 0)
    cl = getattr(module, class_name)

    return cl()


def main():
    logging.basicConfig(
        format='%(asctime)s: %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S')

    opts = parse_command_line()

    bootstrap_servers = opts.bootstrap_servers
    bucket_name = opts.bucket_name
    group_id = getattr(opts, 'reload_consumer_group', None)
    topic = opts.topic
    dump_id = opts.prefix
    transformer = opts.transformer

    with KafkaClient(topic=topic, group_id=group_id,
                     bootstrap_servers=bootstrap_servers) as kafka_client:
        with tempfile.TemporaryDirectory() as tmp_dir:
            local_dir = opts.path or tmp_dir

            if opts.action == 'dump':
                if not dump_id:
                    dump_id = '{:%Y%m%d%H%M%S}'.format(datetime.utcnow())
                    msg = 'Using autogenerated dump id <{}>'
                    logger.info(msg.format(dump_id))
                kafka_client.get_messages(
                    num_messages_to_consume=opts.num_messages,
                    max_package_size_in_msgs=opts.max_messages_per_package,
                    local_dir=local_dir,
                    bucket_name=bucket_name,
                    dry_run=opts.dry_run,
                    dump_id=dump_id)

            elif opts.action == 'reload':
                if not dump_id:
                    dump_id = kafka_client.find_latest_dump_id(bucket_name)
                    logger.info('Using latest dump id <{}>'.format(dump_id))
                cl = get_transformer_class(transformer)

                msg = 'Using class=<{}> to pre process events'
                logger.info(msg.format(type(cl)))
                kafka_client.reload_kafka_server(
                    bucket_name=bucket_name,
                    local_dir=local_dir,
                    dump_id=dump_id,
                    transformer_class=cl)

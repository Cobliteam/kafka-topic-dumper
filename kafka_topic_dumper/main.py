import argparse
import logging
import tempfile
import json
import sys
from datetime import datetime

from kafka_topic_dumper.kafka_client import KafkaClient


logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def parse_command_line():
    parser = argparse.ArgumentParser(description='Simple tool to dump kafka '
                                                 'messages and send it to AWS '
                                                 'S3')

    parser.add_argument('-t', '--topic', default='test',
                        help='Kafka topic to fetch messages from.')

    parser.add_argument('-s', '--bootstrap-servers', default='localhost:9092',
                        help='host[:port] string (or list of host[:port] '
                             'strings) that the consumer should contact to '
                             'bootstrap initial cluster metadata. If no '
                             'servers are specified, will default to '
                             'localhost:9092.')

    parser.add_argument('-b', '--bucket-name', default='kafka-topic-dumper',
                        help='The AWS-S3 bucket name to send dump files.')

    parser.add_argument('-p', '--path', default=None,
                        help='Path to folder where to store local files.')

    parser.add_argument('-x', '--prefix', default=None,
                        help='The name of the folder to dump/reload.')

    cmds = parser.add_subparsers(help='sub-command help')

    dump_cmd = cmds.add_parser('dump',
                               help='Dump mode will fetch messages from kafka '
                                    'cluster and send then to AWS-S3.')

    dump_cmd.add_argument('-n', '--num-messages', default=300, type=int,
                          help='Number of messages to try dump.')

    dump_cmd.add_argument('-m', '--max-messages-per-package', default=100,
                          type=int,
                          help='Maximum number of messages per dump file.')

    dump_cmd.add_argument('-d', '--dry-run', action='store_true',
                          help='In dry run mode, kafka-topic-dumper will '
                               'generate local files. But will not send it to '
                               'AWS S3 bucket.')

    dump_cmd.set_defaults(action='dump')

    reload_cmd = cmds.add_parser('reload',
                                 help='Reload mode will download files from '
                                      'AWS-S3 and send then to kafka.')

    reload_cmd.add_argument('-g', '--reload-consumer-group',
                            default=None,
                            help='Whe reloading a dump of messages that '
                                 'already was in kafka, kafka-topic-dumper '
                                 'will not load it again, it will only reset '
                                 'offsets for this consumer-group.')

    reload_cmd.add_argument('-m', '--multiply-event-report-messages',
                            default=None, type=int,
                            help='When reloading a dump of event-reports '
                                 'messages each message will be produced m '
                                 'times with a new device-id')

    reload_cmd.set_defaults(action='reload')

    opts = parser.parse_args()

    if getattr(opts, 'action', None) is None:
        parser.print_help()
        sys.exit(1)

    print(opts)

    return opts


def identity_processing(row):
    yield row


def demultiplex_status_event(row, m):
    raw_key = row[1]
    raw_value = json.loads(row[2])
    original_esn = raw_value["optionsHeader"]["mobileId"]

    for i in range(m):
        if i == 0:
            yield row
        else:
            key = raw_key + str(i).encode('ascii')
            new_esn = "{}{}".format(original_esn, i)
            raw_value["optionsHeader"]["mobileId"] = new_esn
            value = json.dumps(raw_value)
            yield (None, key, value)

def main():
    logging.basicConfig(
        format='%(asctime)s: %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S')

    opts = parse_command_line()

    bootstrap_servers = opts.bootstrap_servers
    bucket_name = opts.bucket_name
    group_id = getattr(opts, 'reload_consumer_group', None)
    topic = opts.topic
    dump_id = opts.prefix
    multiply = getattr(opts, 'multiply_event_report_messages', None)

    logger.info("multiply=<{}>".format(multiply))

    with KafkaClient(topic=topic, group_id=group_id,
                     bootstrap_servers=bootstrap_servers) as kafka_client:
        with tempfile.TemporaryDirectory() as tmp_dir:
            local_dir = opts.path or tmp_dir

            if opts.action == 'dump':
                if not dump_id:
                    dump_id = '{:%Y%m%d%H%M%S}'.format(datetime.utcnow())
                    msg = 'Using autogenerated dump id <{}>'
                    logger.info(msg.format(dump_id))
                kafka_client.get_messages(
                    num_messages_to_consume=opts.num_messages,
                    max_package_size_in_msgs=opts.max_messages_per_package,
                    local_dir=local_dir,
                    bucket_name=bucket_name,
                    dry_run=opts.dry_run,
                    dump_id=dump_id)

            elif opts.action == 'reload':
                if not dump_id:
                    dump_id = kafka_client.find_latest_dump_id(bucket_name)
                    logger.info('Using latest dump id <{}>'.format(dump_id))

                if multiply is None or multiply == 1:
                    row_preprocessing = identity_processing
                else:
                    row_preprocessing = lambda x: demultiplex_status_event(x, multiply)

                kafka_client.reload_kafka_server(
                    bucket_name=bucket_name,
                    local_dir=local_dir,
                    dump_id=dump_id,
                    row_preprocessing = row_preprocessing)
